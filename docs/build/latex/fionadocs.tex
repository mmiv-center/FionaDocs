%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{FionaDocs}
\date{Aug 12, 2025}
\release{}
\author{Hauke Bartsch, Marek Kociński}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
\sphinxstylestrong{FIONA} is a comprehensive medical imaging data management and processing platform designed for healthcare research environments. The system handles DICOM medical imaging data throughout its entire lifecycle \sphinxhyphen{} from initial acquisition at imaging scanners to final anonymized export for research purposes. Provides DICOM anonymization, quarantine management, and automated transfer from clinical to research PACS systems while ensuring General Data Protection Regulation (GDPR) compliance.

\sphinxAtStartPar
\sphinxstylestrong{The architecture} of the Fiona system can be divided into five layers, including: network layer, processing layer, storage layer, transfer layer and management layer.

\sphinxincludegraphics{mermaid-fa5d0497195ed68246d643b448ad5e1186c6025f.pdf}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Network Layer}: DICOM communication services that receive imaging data from scanners and send to research PACS using standard medical imaging protocols.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Processing Layer}: Core data processing engines that extract metadata, perform anonymization, and execute containerized analysis workflows on medical imaging studies.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Storage Layer}: Organized file system architecture with symbolic links, structured directories, and automated lifecycle management for imaging data and metadata.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transfer Layer}: Secure data distribution system that creates anonymized exports, manages transfer requests, and delivers data to external research repositories.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Management Layer}: Administrative services including health monitoring, configuration management, audit logging, and automated maintenance operations.

\end{itemize}

\sphinxAtStartPar
The system operates as a distributed service with daemon processes, cron jobs, and web applications
working together to provide automated medical imaging research data management.

\sphinxstepscope


\chapter{END USER}
\label{\detokenize{EndUser/index:end-user}}\label{\detokenize{EndUser/index::doc}}
\sphinxAtStartPar
\sphinxstylestrong{For:} Doctors, researchers, medical personnel


\section{Fiona system}
\label{\detokenize{EndUser/index:fiona-system}}
\sphinxAtStartPar
The Fiona system is a comprehensive solution for managing DICOM medical images in a research environment. The system enables automatic reception, processing, anonymization, and transfer of imaging data between Clinical and Research PACS (Picture Archiving and Communication System) systems.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{steve-project}.png}
\caption{Fiona front page.}\label{\detokenize{EndUser/index:id1}}\end{figure}


\section{Research Information System}
\label{\detokenize{EndUser/index:research-information-system}}
\sphinxAtStartPar
The research information system (RIS) of the Western Norway Health Authorities (Helse\sphinxhyphen{}Vest) also called the “Steve Project” is a secure computer system that stores
research data for approved research projects at Haukeland University Hospital and connected hospitals of the Helse Vest region. The project is supported by the
radiology department of \sphinxhref{https://www.helse-bergen.no/en}{Mohn Medical Imaging and Visualization Centre} and the \sphinxhref{https:/mmiv.no}{Haukeland University Hospital} and approved for research project use by \sphinxhref{https://www.helse-vest-ikt.no}{Helse Vest IKT}. The physical location of the data is at the premises of IKT Helse Vest Norway. Dedicated storage area and research software (Sectra, IDS7) provides researchers with
appropriate permission access to their data. All data is stored in a de\sphinxhyphen{}identified format inside the RIS. Maintaining a coupling list is the responsibility of each project
and not part of the functionality of the RIS. Based on the REK/DIPA rules for each project a lifetime tracking of the research data
per project ensures that data can be anonymized based on data sharing requirements, and that data can be deleted at the end of the project phase \sphinxhyphen{} if
required. We suggest that research data is allowed to be fully anonymized at the end of the project and remain in RIS for general research access.
Key features of the RIS include:•
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hosted side\sphinxhyphen{}by\sphinxhyphen{}side with the clinical PACS as an independent installation.

\item {} 
\sphinxAtStartPar
Accepts de\sphinxhyphen{}identified patient identifiers only.

\item {} 
\sphinxAtStartPar
All data is moved through a de\sphinxhyphen{}identification process upon import into RIS.

\item {} 
\sphinxAtStartPar
All data is assigned to one or more specific research projects and the visibility of data is restricted to individuals with project role access rights.

\item {} 
\sphinxAtStartPar
Projects require a valid REK approval, such documentation has to be provided at the start of a project by the project owner.

\item {} 
\sphinxAtStartPar
The project owner can identify additional user accounts that can access the data.

\item {} 
\sphinxAtStartPar
User access to the research PACS is controlled by IKT and requires a valid Haukeland University Hospital user account.

\end{itemize}


\section{New project: creation, setup and access}
\label{\detokenize{EndUser/index:new-project-creation-setup-and-access}}

\subsection{Apply for a new project}
\label{\detokenize{EndUser/index:apply-for-a-new-project}}
\sphinxAtStartPar
In order to apply for a new project on the research information system (research PACS)
please fill out the application form available under “\sphinxstylestrong{Apply for a new research project}”
here: \sphinxurl{https://fiona.local.server.no}.

\sphinxAtStartPar
Additional user access can be requested by the principal investigator of the project under
\sphinxhref{https://fiona.local.server.no/applications/Apply/}{Apply} (\sphinxurl{https://fiona.local.server.no/applications/Apply/}) “\sphinxstylestrong{Apply for access to an existing project}”.

\sphinxAtStartPar
If you encounter any problems with applying for access, contact \sphinxhref{mailto:admin@your-institution.com}{admin@your\sphinxhyphen{}institution.com}.


\subsection{Access to REDCap for structured data}
\label{\detokenize{EndUser/index:access-to-redcap-for-structured-data}}
\sphinxAtStartPar
Our project uses \sphinxhref{https://project-redcap.org}{REDCap} as an electronic data capture solution. Projects on the research information system can receive access to their RedCap project as well as access to the image data viewing (see next section).


\subsection{Access to the “Sectra DMA Forskning” research PACS viewer}
\label{\detokenize{EndUser/index:access-to-the-sectra-dma-forskning-research-pacs-viewer}}
\sphinxAtStartPar
Access to the image data is provided by your \sphinxhref{https://it.department.you.institution.no}{IT Department}. Such access may require a valid local hospital user account and a laptop or PACS workstation that is under control of IT Department. If you contact IT Departmetn ask for the start menu item “\sphinxstylestrong{Sectra DMA Forskning}”. With the program and your hospital username and password you will gain access to the research picture archive and communication system (PACS).

\sphinxAtStartPar
Without access to a specific research project you will not see any data in the research PACS. Each research projects requires specific permissions to become accessible for a user.

\sphinxAtStartPar
The research PACS viewer is using a separate clinical PACS software installation (Sectra IDS7). In order to prevent possible interactions between the clinical and the research PACS only one of the application can run at a given time. You will be logged out of the clinical PACS if you start the research PACS viewer.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.75]{{sectra-2}.png}
\caption{Forskning PACS log\sphinxhyphen{}inn window to view image data by project.}\label{\detokenize{EndUser/index:id2}}\end{figure}


\section{Submit data to the Research Information System}
\label{\detokenize{EndUser/index:submit-data-to-the-research-information-system}}
\sphinxAtStartPar
\sphinxstylestrong{The Research Information System} (RIS) contains two components. First, image data is stored in the Sectra DMA Forskning \sphinxhyphen{} an image viewer with a vendor neutral archive (VNA). Second, all meta\sphinxhyphen{}data is stored in table format in an electronic data capture system REDCap, that is Fiona server on port 4444, (\sphinxurl{https://fiona.local.server.no:4444}). Sending image data will create the appropriate entries in \sphinxhref{https://fiona.local.server.no:4444}{RedCap (Fiona, port 4444)}. Additional data collection instruments can be set up there and used to capture assessments, consent/assent and results from automated image processing. All image data is assigned to a project to allow for project specific data views for each research information user.

\sphinxAtStartPar
The basic steps to submit data are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Send DICOM studies to “HBE Fiona” or “Fiona” (modality station)

\item {} 
\sphinxAtStartPar
\sphinxhref{https://fiona.local.server.no/applications/Assign/}{Assign} to project on \sphinxurl{https://fiona.local.server.no/applications/Assign/}.

\end{enumerate}

\sphinxAtStartPar
In step 1 data arrives in a \sphinxstylestrong{quarantine} location. In step 2 each DICOM study needs to be \sphinxstylestrong{assigned to project}, pseudonymized participant identifier and event name before it will be forwarded to the research PACS and becomes visible to the project users.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{fiona-assign-view}.png}
\end{figure}



\sphinxAtStartPar
\sphinxstylestrong{Setup of a new project}

\sphinxAtStartPar
The project needs to exist on the research information system before participant data is collected. After a successful setup your project and event names should appear in the Assign application.


\subsection{\sphinxstylestrong{How to add image data}}
\label{\detokenize{EndUser/index:how-to-add-image-data}}
\sphinxAtStartPar
The end\sphinxhyphen{}point for images is \sphinxhref{https://fiona.local.server.no}{Fiona} (\sphinxurl{https://fiona.local.server.no}):
\begin{itemize}
\item {} 
\sphinxAtStartPar
AETitle: \sphinxhref{AETtitle}{Application Entity Title}

\item {} 
\sphinxAtStartPar
\sphinxhref{fiona:local.ip}{IP: xxx.xxx.xxx.xxx}

\item {} 
\sphinxAtStartPar
Port: 11112

\end{itemize}

\sphinxAtStartPar
Images that arrive at this endpoint are added to a quarantine system (FIONA, \sphinxurl{https://fiona.local.server.no:4444}) running the REDCap software. Automatic routing rules (stored in REDCap) are used to anonymize and forward the data to the image storage. If such routing has not been set up, the “Assign” application (see below) needs to be used to forward individual studies based on pre\sphinxhyphen{}existing patient ID lists.

\sphinxAtStartPar
From Sectra Production you can send image data to the endpoint “HBE Fiona”. Modality stations might also have the “FIONA” endpoint setup. If the data is already anonymized and has a de\sphinxhyphen{}identified PatientName/PatientID entry that indicates the project the FIONA system will attempt to de\sphinxhyphen{}identify (pseudonymization) further DICOM tags and forward the images to IDS7 (may take minutes). No further action is needed. If you suspect this did not work, see the corresponding section about the representation of transfers in REDCap.

\sphinxAtStartPar
Image data that contains patient information cannot be automatically assigned to the appropriate project as there is only a single endpoint for FIONA shared by all projects. To assign participants correctly to projects and de\sphinxhyphen{}identified participant identifiers a user can perform the assignment to project, participant ID and event name in the “Assign” web application.

\sphinxAtStartPar
If the participant identifiers do not exist yet user may add new project specific identifiers in “Assign”. Such identifiers need to follow the naming rules for a project and are verified using regular expression pattern specific for each project.

\sphinxAtStartPar
The web application for the assignment of scans forwarded to HBE Fiona is available at: \sphinxurl{https://fiona.local.server.no/applications/Assign/}.

\sphinxAtStartPar
On the Assign website look for your forwarded study. It should appear in about 15 min.
Identify the correct scan using the Accession Number (Undersøkelse\sphinxhyphen{}ID) or the date and time
of the scan. Select your project from the drop\sphinxhyphen{}down. This will fill in the list of patient names
and event names. Select the correct patient name and the event this study belongs to. After
a couple of seconds a new button appears below the study entry. Use it to select and
confirm the assignment. This will forward a de\sphinxhyphen{}identified version of the study data to “Sectra
Forskning”. If you do not assign your data on Assign they will not be forwarded. After a
couple of days (7 days) such data will disappear from the list. Send an email to \sphinxhref{mailto:admin@your-institution.com}{Local Fiona Admin} to request a resend.

\sphinxAtStartPar
\sphinxstylestrong{Verification steps}

\sphinxAtStartPar
After data arrived at the research PACS a verification step should ensure that all images have been received at the quarantine on FIONA and have been forwarded to research PACS. This can be done by comparing the number of images on the sending station with the number of images in IDS7.

\sphinxAtStartPar
Furthermore the import step will also attempt to de\sphinxhyphen{}identify secondary capture images with burned in image information. This process is fully automated and can result in false positive and occasionally false negative results. After a review of the data in IDS7 the user may decide which secondary image series are “safe” to exclude from the pixel rewriting on import. For example a secondary capture series from DTI may not contain any burned in names or identifying numbers or dates. Such image series can be removed in REDCap from further pixel anonymization.

\sphinxAtStartPar
If the number of images on FIONA does not correspond to the number of images available cache previous assignments and automatically forward such images to the research PACS using the previously defined project, patient identifier and event name.

\sphinxAtStartPar
\sphinxstylestrong{Features for data migration}

\sphinxAtStartPar
The \sphinxhref{https://fiona.local.server.no/applications/Assign/}{Assign} web\sphinxhyphen{}application allows users to upload a coupling list that maps the accession
number (Undersøkelse\sphinxhyphen{}ID) of the study to the pseudonymized participant identifier. Such
mappings must be uploaded before the first image study of the project has been forwarded
to FIONA. Incoming DICOM studies in FIONA that match entries in the coupling list will
automatically be assigned to the project.

\sphinxAtStartPar
\sphinxstylestrong{How to handle errors?}

\sphinxAtStartPar
Correcting errors during data import are not difficult to fix. Try to follow up on such errors
on an ongoing basis. The quarantine FIONA station may have still have a copy of the data in
its cache which simplifies the process. Contact \sphinxhref{mailto:admin@your-institution.com}{Local Fiona Admin} in such cases and ask for help. This will allow you to fix issues such as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Wrong assignment of participant identifiers to DICOM studies

\item {} 
\sphinxAtStartPar
Wrong assignment of event names to DICOM studies

\item {} 
\sphinxAtStartPar
Missing images or image series for existing DICOM studies

\item {} 
\sphinxAtStartPar
Missing entries for DICOM studies on “Assign”.

\end{itemize}


\section{Export image data from research PACS}
\label{\detokenize{EndUser/index:export-image-data-from-research-pacs}}
\sphinxAtStartPar
Data in the research PACS is secured by generic procedures during data import that delete or rewrite some DICOM tags, changes dates and replaces unique identifiers. A documentation of this process is available on the GitHub repository of the projects for removal of DICOM meta\sphinxhyphen{}tags: \sphinxhref{https://github.com/mmiv-center/DICOMAnonymizer}{DICOMAnonymizer}, and for the removal of burned in image information: \sphinxhref{https://github.com/mmiv-center/RewritePixel}{RewritePixel}.

\sphinxAtStartPar
Data stored in the research PACS is therefore in general suited for data sharing IF pseudonymized data is allowed. In order to support users with the task of data pseudonymization the research information system provides the “Review” web application that lists all existing DICOM tags in a research project (\sphinxurl{https://fiona.local.server.no}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Pseudonymized data is defined here as data for which a coupling list exists somewhere in the universe. This is in contrast to anonymized data where such a list does not exist and can also not be created.
\end{sphinxadmonition}

\sphinxAtStartPar
Further de\sphinxhyphen{}identification procedures might require changes to image data such as face stripping, removal of outer ear tissue, cortical folding pattern, etc.. Such potential sources of information for re\sphinxhyphen{}identification have been proposed in the literature but actual attacks based on them have not recently been documented. Better documented and perhaps more relevant are re\sphinxhyphen{}identification using spreadsheet data where external sources are linked to the projects data to discover the supposedly hidden identity of the research participants. For example it might be possible to link Gender, day of birth and the hospital name to a real participant name using a birth or voting registry.

\sphinxAtStartPar
\sphinxstylestrong{Export using IDS7}

\sphinxAtStartPar
The image data from a study can be exported from the research PACS using a right\sphinxhyphen{}click menu entry available in the Informasjonsvindu “Exporter til medium”. Such exports will generate either a derived patient ID \textendash{} if an Anonymization Profile is selected or a faithful copy of the data with all pseudonymized DICOM tags intact.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This export does not prevent re\sphinxhyphen{}identification. Specifically the PatientID field is created from the pseudonymized ID used in the research PACS and therefore not random.
\end{sphinxadmonition}

\sphinxAtStartPar
The export is also case\sphinxhyphen{}by\sphinxhyphen{}case, which is tedious if many data need to be exported. The export will also result in directory names that do not reflect the research project structure as participant identifier \textendash{} event name \textendash{} modality \textendash{} image series. It may be advantageous to export from IDS7 if a single image study needs to be shared without special requirements. Such export folders will also contain an image viewer.

\sphinxAtStartPar
\sphinxstylestrong{Export to project specific formats, NIfTI and zip\sphinxhyphen{}files}

\sphinxAtStartPar
The research information system supports a separate export facility that is more suited to implement project specific de\sphinxhyphen{}identification. Such export requirements include specific DICOM value changes (replacing underscores with dashes), adding birth date information back, formatting and cleaning of series descriptions, zip\sphinxhyphen{}file exports with specific folder structures etc.. This export is appropriate if the receiving institution has specific requirements on how data should be shared.

\sphinxAtStartPar
Request access to the specialized data exports for your project from \sphinxhref{mailto:admin@your-institution.com}{Local Fiona Admin}. Provide your export specification and we will implement your anonymization scheme and make it available to you and other researchers. As an example the “Export” application currently supports the export in NIfTI formats (using dcm2niix) and the export in several zip\sphinxhyphen{}file formats.


\section{End\sphinxhyphen{}user contract}
\label{\detokenize{EndUser/index:end-user-contract}}
\begin{sphinxadmonition}{attention}{Attention:}
\sphinxAtStartPar
The following text is from the Apply website for the Steve system. Please check this page for updates to the wording.
\end{sphinxadmonition}

\sphinxAtStartPar
By creating a project on the research information system you agree to the following:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
All data stored in the RIS belongs to the research project owner represented by the PI of the project. Adding and verifying added data to the RIS is the responsibility of the project owner. The RIS team will help research projects to automate this process.

\item {} 
\sphinxAtStartPar
Sensitive participant information needs to be stored under a separate account and needs to be accessible to authorized (data\sphinxhyphen{}manager and above) user accounts only. All other research data is stored as de\sphinxhyphen{}identified data (pseudonymized, with external coupling list) or in an anonymized format. This restriction includes sensitive information such as Norwegian identification numbers, real names or parts of real names, other birth certificate information and initials. It is the responsibility of the project to review the result of the de\sphinxhyphen{}identification procedures implemented by the RIS team on image meta\sphinxhyphen{}data using \sphinxurl{https://fiona.local.server.no/applications/ReviewDICOMTags/} and the result of the detection and removal of burned in image data (IDS7). The project will inform the RIS team in a timely manner if the pseudonymization procedure of the RIS team needs to be updated. This restriction is in place to allow for the largest possible user base for the RIS including PhD students and external collaborators.

\item {} 
\sphinxAtStartPar
All research data is stored as part of RIS projects identified by a project name  of 5\sphinxhyphen{}20 characters. Users can gain access to the data upon request from the project PI or an appointed representative of the PI.

\item {} 
\sphinxAtStartPar
Projects are expected to utilize best\sphinxhyphen{}practises for data handling such as  accounts based on roles like data\sphinxhyphen{}entry (add data only) and data\sphinxhyphen{}manager (change data, export data). Personally identifying fields have to be marked as such (Identifier? field of the instrument designer) and data access groups shall be used for multi\sphinxhyphen{}site project setups.

\end{enumerate}

\sphinxAtStartPar
5. Projects will undergo a short review from the RIS team before they are moved by the RIS team from development mode into production mode for data capture. This review may generate suggestions for the project on how to implement best practices for longitudinal data captures, missing validation and the use of additional software features. All research data is collected and stored with a valid REK approval for the time period specified in the REK approval. The REK approval is required at the time that the RIS project is created. Any change of the REK approval start and end dates need to be
reported to the RIS team. At the end of the project period data can be either a) deleted or b) fully anonymized (suggested choice). It is up to the project to inform the RIS team about the correct way of handling the data at the end of the project. By default we will assume that data needs to be deleted. Based on the project end date (REK) the RIS team will inform the PI of the project of a pending change of the project status to the archive state. An archive state project will not allow for further data entry, or changes to captured data. After a period of about 1 year the project data will be exported and provided to the project PI for download. An archived project can be deleted by the RIS team after an unspecified time period. If the project data can be fully anonymized, the RIS team may create a copy of the data with new participant identifiers (without a coupling list). After a re\sphinxhyphen{}import a fully anonymized version of the project data can become accessible to other RIS users. The original project
data will change to archive state, a copy is provided to the projects PI and the data can be deleted by the RIS team after about 1 year.


\section{Sensitive Data Projects}
\label{\detokenize{EndUser/index:sensitive-data-projects}}

\subsection{Separation of Sensitive Information and Data}
\label{\detokenize{EndUser/index:separation-of-sensitive-information-and-data}}
\sphinxAtStartPar
A sensitive data project is one that is used to capture human subject data and in general will require  REK (regional ethics board approval). In order to setup such a project in REDCap we suggest the follow structure and features of REDCap to be used. These recommendations have been generated based
on discussions in relevant risk assessments.

\sphinxAtStartPar
All sensitive data should be stored in a separate REDCap “ID” project including Norwegian Identification Numbers, names or parts of names, addresses and full birth dates (see Figure 1). This project should have its own roles of “Data Manager”, “Data Entry”, and “Controller”. People with permission to access and/or edit this information can use this database to keep contact information up\sphinxhyphen{}to\sphinxhyphen{}date and to enroll new participants into the study. Each participant should be assigned a pseudonymized ID in the sensitive data project that links the entry to the corresponding participant in the data project. Examples for this ID are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\textless{}project name\textgreater{}\sphinxhyphen{}\textless{}site number\textgreater{}\sphinxhyphen{}0001,

\item {} 
\sphinxAtStartPar
\textless{}project name\textgreater{}\sphinxhyphen{}\textless{}site number\textgreater{}\sphinxhyphen{}0002,

\item {} 
\sphinxAtStartPar
etc..

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{redcap-sensitive-data}.png}
\caption{Sensitive data projects should be split into a REDCap project for data (using pseudonymized ids) and a REDCap project for sensitive data including the coupling list.}\label{\detokenize{EndUser/index:id3}}\end{figure}

\sphinxAtStartPar
All other data should be stored in a separate REDCap “Data” project using the pseudonymized articipant ID as a “record\_id” (first field in the study).

\sphinxAtStartPar
\sphinxstylestrong{User rights management}

\sphinxAtStartPar
When a project leader / principal investigator (PI) is given a REDCap account and project, they are given “project owner” roles. The project owner can then provide access to project members in “roles”. A role defines a given set of custom permissions which defines the user’s access to data, export permissions and ability to make changes to data.

\sphinxAtStartPar
Each project can have predefined roles. We recommend the predefined roles:
\begin{itemize}
\item {} 
\sphinxAtStartPar
“Data Manager” (ability to change study design, export),

\item {} 
\sphinxAtStartPar
“Data Entry” (add, change, or delete data),

\item {} 
\sphinxAtStartPar
“Controller” to define roles for data viewing, editing, and deleting records.

\end{itemize}

\sphinxAtStartPar
In more complex cases, different access settings can be given on different forms in the study (see also API access with REDCap). Individual users are assigned to project roles as part of gaining access to one project.

\sphinxAtStartPar
The user rights management is the responsibility of the project owner and/or the users they add to the project with User Rights access. User roles should be set at the lowest access level that is necessary (e.g., export rights only for users who need this permission). Access to the project should be reviewed regularly and personnel who no longer require access need to be removed from the project.

\sphinxAtStartPar
\sphinxstylestrong{User rights \textendash{} multi\sphinxhyphen{}center projects}

\sphinxAtStartPar
In a project where several institutions participate with their own project participants (several hospitals etc.) each group of participants should be assigned to a separate “data access group”. This functionality allows records in a study to be part of the user rights management. A user with access to a single data access group can only see participants that belong to this group. If this user creates a new participant, they will be automatically assigned to the group.

\sphinxAtStartPar
\sphinxstylestrong{How to handle Email Addresses in Data Projects}

\sphinxAtStartPar
Email addresses are special identifying fields that can be stored in data projects for the purpose of creating automated invites for participants to fill out forms from home. In projects that use this feature email fields need to be present in the data project in order to allow for email distribution to participants.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Add such email fields to a separate instrument of the REDCap data project and mark the instrument as viewable by specific roles only (like Data Managers).

\item {} 
\sphinxAtStartPar
Mark the email field as an “Identifier” field to prevent export of the field’s data by user  of roles that cannot view sensitive fields.

\item {} 
\sphinxAtStartPar
Add the Action Tag “@PASSWORDMASK” to the field to prevent accidental viewing of the fields values if the instrument is displayed on screen.

\item {} 
\sphinxAtStartPar
Add a field validation as “Email” to prevent some miss\sphinxhyphen{}typing of emails.

\end{enumerate}


\subsection{Randomization}
\label{\detokenize{EndUser/index:randomization}}
\sphinxAtStartPar
Randomized studies have can remove biases caused by selection of participants for specific arms in a study. Such biases can prevent a fair assessment of a treatment option. The randomization feature of REDCap allows users to upload a randomization table that has been externally created before the start of the study \textendash{} usually by the statistician of the project. After participants are enrolled into the study the randomization entries for that person are “opened” and the choice of the randomization is stored in the project.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{sensitive-data-randomization}.png}
\caption{Basics of the randomization workflow in REDCap.}\label{\detokenize{EndUser/index:id4}}\end{figure}


\subsection{e\sphinxhyphen{}Consent}
\label{\detokenize{EndUser/index:e-consent}}
\sphinxAtStartPar
In an e\sphinxhyphen{}Consent workflow the basics of the paper informed consent are maintained. An electronic consent document is created based on the approved language and design of the paper consent using HTML features in REDCap. The solution supports signature fields (stored as images) and creates resulting PDF (paper) versions of the consent as well as electronic versions of the consent. The following figures show some of the setup and resulting documentation that is created in the solution.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{sensitive-data-e-consent}.png}
\caption{Setup of e\sphinxhyphen{}Consent in REDCap with identification of typed information for participant name and signature fields.}\label{\detokenize{EndUser/index:id5}}\end{figure}

\sphinxAtStartPar
We suggest exporting e\sphinxhyphen{}Consent documents and to store them centrally by the project administration outside of REDCap.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{sensitive-data-e-consent-user-view}.png}
\caption{Example e\sphinxhyphen{}Consent document structure (left) with (right) visual representation and signature (middle, bottom).}\label{\detokenize{EndUser/index:id6}}\end{figure}

\sphinxAtStartPar
As informed consent document contain the name of the signatory and the one countersigning the informed consent process the e\sphinxhyphen{}Consent workflow should be implemented in the sensitive data (ID) REDCap project.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{sensitive-data-e-consent-in-person}.png}
\caption{Internal documentation of e\sphinxhyphen{}Consent in REDCap. Signatures have been captured and tracked. A paper version for export is available.}\label{\detokenize{EndUser/index:id7}}\end{figure}


\subsection{Automatic data exports from REDCap}
\label{\detokenize{EndUser/index:automatic-data-exports-from-redcap}}
\sphinxAtStartPar
Data may be exported from REDCap using the REDCap API, a technical interface to automate the export of project and participant information using scripting. To provide such access a dedicated user\sphinxhyphen{}account “api\_\textless{}real username\textgreater{}” should be created which is specific for a single project. Configure the account with a limited set of read permissions to specific fields or instruments using a new API role. The REDCap API will borrow these restrictive permissions for controlled access.

\sphinxAtStartPar
Setup: An administrator can generate an API “token” for this account and share the token and examples of accessing the resource (curl\sphinxhyphen{}based access) with the user.

\sphinxAtStartPar
Any change in the role of the \textless{}real username\textgreater{} should also apply to the connected API account. Specifically loosing access to the project should be implemented for both \textless{}real username\textgreater{} and api\_\textless{}real username\textgreater{}.


\subsection{Steps at the end of a REDCap project}
\label{\detokenize{EndUser/index:steps-at-the-end-of-a-redcap-project}}
\sphinxAtStartPar
REDCap is a tool for data collection. At the end of data capture projects using REDCap receive a notification of study end. At this point projects may provide updated REK information (extension of data capture notice). If no such notice is received REDCap projects will:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lock all data participants (no further update/add).

\item {} 
\sphinxAtStartPar
Provide a copy of the REDCap project (CDISC format) to the project’s principal investigator or delegate.

\item {} 
\sphinxAtStartPar
Provide a copy of the project data (CSV) and data dictionary (PDF) to the principal investigator or delegate.

\item {} 
\sphinxAtStartPar
Request a confirmation that project data (CDISC and CSV) have been received by the project.

\item {} 
\sphinxAtStartPar
Permanently delete all project data.

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.800\linewidth]{{redcap-end-of-project}.png}
\caption{End\sphinxhyphen{}of\sphinxhyphen{}project tracking for REDCap projects}\label{\detokenize{EndUser/index:id8}}\end{figure}

\sphinxAtStartPar
This process will be documented in the REDCap project tracking project “DataTransferProjects”, the project management tool with information on identity of the person requesting project removal and confirmations for all steps of the project removal process.


\section{External resources}
\label{\detokenize{EndUser/index:external-resources}}

\subsection{Git\sphinxhyphen{}Hub Repositories}
\label{\detokenize{EndUser/index:git-hub-repositories}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/mmiv-center/FionaDocs}{Fiona Documentation Repository} \sphinxhyphen{} \sphinxurl{https://github.com/mmiv-center/FionaDocs}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/mmiv-center/DICOMAnonymizer}{DICOMAnonymizer} \sphinxhyphen{} \sphinxurl{https://github.com/mmiv-center/DICOMAnonymizer}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://github.com/mmiv-center/RewritePixel}{RewritePixel} \sphinxhyphen{} \sphinxurl{https://github.com/mmiv-center/RewritePixel}

\end{enumerate}


\subsection{REDCap}
\label{\detokenize{EndUser/index:redcap}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxhref{https://project-redcap.org}{REDCap} \sphinxhyphen{} \sphinxurl{https://project-redcap.org}

\end{enumerate}

\sphinxstepscope


\chapter{ADMINISTRATION}
\label{\detokenize{ServerAdmin/index:administration}}\label{\detokenize{ServerAdmin/index::doc}}
\sphinxAtStartPar
\sphinxstylestrong{For:} IT administrators, DevOps

\sphinxstepscope


\chapter{ARCHITECTURE}
\label{\detokenize{Architecture/index:architecture}}\label{\detokenize{Architecture/index::doc}}
\sphinxAtStartPar
\sphinxstylestrong{For:} Developers, system architects

\sphinxAtStartPar
\sphinxstylestrong{FIONA System Architecture}

\sphinxAtStartPar
A detailed system architecture including all system components is presented below

\begin{figure}[htbp]
\centering
\capstart

\sphinxincludegraphics{mermaid-d232899be5c16d633f58ea5dd0508da7ba94305f.pdf}
\caption{FIONA layered Architecture}\label{\detokenize{Architecture/index:id1}}\end{figure}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Dowlnoad image as PDF}}


\section{Setup}
\label{\detokenize{Architecture/index:setup}}
\sphinxAtStartPar
To setup the system some mandatory information must be set set in \sphinxcode{\sphinxupquote{/data/config/conf.json}} file. Fill values appropriate to your serwer as shown in example json file.
\sphinxSetupCaptionForVerbatim{System configuration example settings}
\def\sphinxLiteralBlockLabel{\label{\detokenize{Architecture/index:config-json}}}
\fvset{hllines={, 3, 5,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{p}{\PYGZob{}}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}DICOMIP\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}dicom\PYGZus{}server\PYGZus{}ip\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}DICOMPORT\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}dicom\PYGZus{}port\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}DICOMAETITLE\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}FIONA\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}SCANNERIP\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}scanner\PYGZus{}ip\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}SCANNERPORT\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}scanner\PYGZus{}port\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}SCANNERAETITLE\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}scanner\PYGZus{}application\PYGZus{}entity\PYGZus{}title\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}SCANNERTYPE\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}scanner\PYGZus{}device\PYGZus{}type\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}MPPSPORT\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}modality\PYGZus{}performed\PYGZus{}procedure\PYGZus{}step\PYGZus{}port\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}SERVERUSER\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}server\PYGZus{}user\PYGZus{}name\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}DAICSERVER\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}daic\PYGZus{}server\PYGZus{}ip\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}PFILEDIR\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/path/to/your/profile/directory\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}CONNECTION\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}DATADIR\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}/path/to/data/dir\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}LOCALTIMEZONE\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}local\PYGZus{}zone\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}PROCESSING\PYGZus{}USER\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}processing\PYGZus{}user\PYGZus{}name\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}PROJECTTOKEN\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}project\PYGZus{}token\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{    }\PYG{n+nt}{\PYGZdq{}Authentication\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}
\PYG{+w}{	}\PYG{n+nt}{\PYGZdq{}Table\PYGZhy{}based\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{\PYGZob{}}\PYG{+w}{ }\PYG{n+nt}{\PYGZdq{}enabled\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{+w}{ }\PYG{p}{\PYGZcb{},}
\PYG{+w}{	}\PYG{n+nt}{\PYGZdq{}LDAP\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{p}{[}
\PYG{+w}{	    }\PYG{p}{\PYGZob{}}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}enabled\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+m+mi}{0}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}connection\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}certificate\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}username\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}password\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}query\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}\PYG{p}{,}
\PYG{+w}{		}\PYG{n+nt}{\PYGZdq{}dn\PYGZdq{}}\PYG{p}{:}\PYG{+w}{ }\PYG{l+s+s2}{\PYGZdq{}your\PYGZus{}value\PYGZdq{}}
\PYG{+w}{	    }\PYG{p}{\PYGZcb{},}
\PYG{+w}{	}\PYG{p}{]}
\PYG{+w}{    }\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}
\sphinxresetverbatimhllines


\section{Folder and File structure}
\label{\detokenize{Architecture/index:folder-and-file-structure}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
/home/processing/
|          └── bin/
│               ├── anonymizeAndSend.py
│               ├── clearExports.sh
│               ├── clearOldFiles.sh
│               ├── clearStaleLinks.sh
│               ├── createTransferRequestsForProcessed.py
│               ├── createTransferRequests.py
│               ├── populateAutoID.py
│               ├── populateIncoming.py
│               ├── populateProjects.py
│               └── utils/
│                      ├── getAllPatients2.sh
│                      ├── parseAllPatients.sh
│                      ├── resendProject.py
│                      ├── whatIsInIDS7.py
│                      └── whatIsNotInIDS7.py
│
/var/
  └── www/
       └── html/
             ├── applications/
             │          ├── Assign/
             │          │     └── php
             |          |          └── removeOldEntries.sh
             │          ├── Attach/
             │          │     └── process\PYGZus{}tiff.sh
             │          ├── Exports/
             │          │     └── php
             |          |          └── createZipFileCmd.php
             │          ├── User/
             │          │     └── asttt/
             │          │            └── code/
             │          │                  └── cron.sh
             │          └── Workflows/
             │                 └──php
             |                    └── runOneJob.sh
             │
             └── server/
                    ├── bin/
                    |    ├── heartbeat.sh
                    |    ├── processSingleFile3.py
                    |    ├── sendFiles.sh
                    |    └── storectl.sh
                    |
                    └── utils/
                          └── s2m.sh
\end{sphinxVerbatim}


\section{Components}
\label{\detokenize{Architecture/index:components}}
\sphinxstepscope


\subsection{anonymizeAndSend.py}
\label{\detokenize{Architecture/scripts/anonymizeAndSend:anonymizeandsend-py}}\label{\detokenize{Architecture/scripts/anonymizeAndSend::doc}}
\sphinxAtStartPar
This Python script processes DICOM medical imaging transfer requests by anonymizing patient data and sending anonymized studies to a research PACS system. The script reads JSON transfer requests, applies project\sphinxhyphen{}specific anonymization rules including burned\sphinxhyphen{}in image detection and pixel data rewriting, then sends the processed data via DICOM protocol while updating REDCap with transfer status. It handles various project features like presentation state removal, secondary capture filtering, and maintains audit trails of all processing operations.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-e5c394312c2dd9c356bebe6210bab34ccf563103.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data flow diagram}

\sphinxincludegraphics{mermaid-a7c975261bb1dfeac5f51613f24cda5e5d667115.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/transfer\_requests/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/transfers\_done/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/transfers\_fail/}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Temporary processing:}\begin{itemize}
\item {} 
\sphinxAtStartPar
System temp directories via \sphinxcode{\sphinxupquote{tempfile.TemporaryDirectory()}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
There is no docstring.

\sphinxAtStartPar
2025.06.17 mk

\sphinxstepscope


\subsection{clearExports.sh}
\label{\detokenize{Architecture/scripts/clearExports:clearexports-sh}}\label{\detokenize{Architecture/scripts/clearExports::doc}}
\sphinxAtStartPar
This bash script manages disk space by automatically deleting old export files when disk usage exceeds 80\%. It first removes temporary directories older than 1000 minutes, then systematically deletes the oldest ZIP files in \sphinxcode{\sphinxupquote{/export2/Export/files/}} until disk usage drops to 80\% or below. The script provides timestamped logging throughout the cleanup process to track which files are being removed.

\sphinxAtStartPar
\sphinxstylestrong{Related File}

\sphinxincludegraphics{mermaid-f5123fe76c0421b1454652bc84ffa3c8a2af854d.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-484de5d34ce75b941377d91d6ed87378fe7ca762.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export2/Export/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export2/Export/files/}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Files are deleted (no output directories)

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
find . \sphinxhyphen{}type d \sphinxhyphen{}maxdepth 1 \sphinxhyphen{}printf ‘\%T+ “\%p”n’ | sort | less

\sphinxAtStartPar
delete oldest files until we have at least 20\% free space on this partition

\sphinxstepscope


\subsection{clearOldFiles.sh}
\label{\detokenize{Architecture/scripts/clearOldFiles:clearoldfiles-sh}}\label{\detokenize{Architecture/scripts/clearOldFiles::doc}}
\sphinxAtStartPar
This bash script automatically manages disk space by deleting oldest archive directories when partition usage exceeds 80\%. It monitors the \sphinxcode{\sphinxupquote{/data/}} partition and systematically removes directories from \sphinxcode{\sphinxupquote{/data/site/archive/}} in chronological order (oldest first) until free space reaches at least 20\%. The script also removes corresponding directories in \sphinxcode{\sphinxupquote{/data/site/raw/}} based on extracted UIDs from archive directory names.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-1d195bba50838661402b0157c929359c39d5d57f.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-ad5c8f81b3f3e45af647710ec15b363dfb2e4076.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input folder:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/archive/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/}}

\end{itemize}

\end{description}

\item {} 
\sphinxAtStartPar
Output folder: No output directories, delete only.

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
find . \sphinxhyphen{}type d \sphinxhyphen{}maxdepth 1 \sphinxhyphen{}printf ‘\%T+ “\%p”n’ | sort | less

\sphinxAtStartPar
delete oldest files until we have at least 20\% free space on this partition

\sphinxAtStartPar
TODO: delete links in /data/site/participants/

\sphinxAtStartPar
TODO: delete links in /data/site/srs/

\sphinxstepscope


\subsection{clearStaleLinks.sh}
\label{\detokenize{Architecture/scripts/clearStaleLinks:clearstalelinks-sh}}\label{\detokenize{Architecture/scripts/clearStaleLinks::doc}}
\sphinxAtStartPar
This bash script performs maintenance on image archive directories by cleaning up broken symbolic links and empty folders. It processes three specific directories (\sphinxcode{\sphinxupquote{/data/site/raw/}}, \sphinxcode{\sphinxupquote{/data/site/participants/}}, \sphinxcode{\sphinxupquote{/data/site/srs/}}) to remove stale symbolic links that no longer point to valid files, delete empty directories, and clean up orphaned JSON metadata files. The script ensures data integrity by maintaining a clean directory structure in the archive system.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-c4d47ff9912e1443edc348d8e0cb64cffd0af898.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-15db36f85fa70977408bdd5a63fc45feccb1c596.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input/Processing Directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/participants/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/srs/}}

\end{itemize}

\end{description}

\item {} 
\sphinxAtStartPar
Output: The same directories (cleaned and maintained)

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Delete links that do not point to real images in archive.

\sphinxAtStartPar
Just to be sure we have a clear /data/site/raw folder we should remove broken symbolic link and empty folders.

\sphinxstepscope


\subsection{createTransferRequests.py}
\label{\detokenize{Architecture/scripts/createTransferRequests:createtransferrequests-py}}\label{\detokenize{Architecture/scripts/createTransferRequests::doc}}
\sphinxAtStartPar
This Python script retrieves medical imaging transfer records from a REDCap database via API and creates JSON transfer request files for studies that need to be transferred. It checks for new image series by comparing transfer dates with file modification times in the raw data directory. The script only creates transfer requests for records that have complete information (project name, transfer name) and haven’t been processed yet.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-717f51467b776303143659f48be2cbf9d7e0783c.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-ffc67bf55700d5f0daa4fb7ce3f63e20402d1847.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/\{study\_instance\_uid\}/}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output directories:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/transfer\_requests/}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
There is no docstring.

\sphinxstepscope


\subsection{createTransferRequestsForProcessed.py}
\label{\detokenize{Architecture/scripts/createTransferRequestsForProcessed:createtransferrequestsforprocessed-py}}\label{\detokenize{Architecture/scripts/createTransferRequestsForProcessed::doc}}
\sphinxAtStartPar
This script automatically detects processed medical imaging series that need to be forwarded to research PACS systems by identifying series with \sphinxcode{\sphinxupquote{StudyInstanceUIDs}} that contain dots (indicating they’re new/processed data). It retrieves transfer requests from REDCap, modifies DICOM metadata to match original study identifiers, and forwards the data either directly to storage or through anonymization processes depending on the project type. The script also extracts structured report data for specific projects like \sphinxcode{\sphinxupquote{Transpara}} studies and \sphinxcode{\sphinxupquote{NOPARK}}.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-5b957df7d5372c358e1c9450500d461cb7b9e382.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-2fe924ffde769d9d81e8be3bc88c770a4f91b309.pdf}

\sphinxAtStartPar
Data paths:
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/*/}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Research PACS server}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/logs/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Temporary directories (created dynamically)}}.

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
There is no docstring.

\sphinxstepscope


\subsection{createZipFileCmd.php}
\label{\detokenize{Architecture/scripts/createZipFileCmd:createzipfilecmd-php}}\label{\detokenize{Architecture/scripts/createZipFileCmd::doc}}
\sphinxAtStartPar
This PHP script is a command\sphinxhyphen{}line cron job that processes medical imaging export requests by creating ZIP archives of DICOM studies. It pulls DICOM data from an imaging server, applies anonymization transformations based on project\sphinxhyphen{}specific rules, optionally converts to NIFTI format, and packages the results into downloadable ZIP files. The script handles secure email delivery with password protection and supports export to TSD (Norwegian research data storage).

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-275ee02d8ef939eb15788e35ac3845a7b84aec30.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-3fb3e0068939e25580676a2a3a11e7fc16a2b0b5.pdf}

\sphinxAtStartPar
Data pahts
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Exports/php/prepared\_downloads\_list.jobs}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/config.json}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Exports/php/tokens.json}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export2/Export/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export2/Export/files/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/logs/}},

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Exports/php/execMeasurements.json}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
This is a docstring.

\sphinxstepscope


\subsection{cron.sh}
\label{\detokenize{Architecture/scripts/cron:cron-sh}}\label{\detokenize{Architecture/scripts/cron::doc}}
\sphinxAtStartPar
This script is an automation scheduler that reads event\sphinxhyphen{}action links from a JSON configuration file (\sphinxcode{\sphinxupquote{/var/www/html/applications/User/asttt/code/links.json}}) and executes corresponding triggers and actions. It processes each link by checking if the associated event trigger conditions are met, and if so, executes the corresponding action script. The script supports email notifications with optional CSV attachments and uses temporary directories for processing outputs. It’s designed to run as a cron job for automated task execution based on predefined event\sphinxhyphen{}action mappings.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-8f5359b8346b658f99c25c8f01816ebe05e8b0fc.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-26d0768268ee9bdd2aaa82499f255bd1a12bca15.pdf}

\sphinxAtStartPar
Data Paths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/User/asttt/code/links.json}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/User/asttt/code/events/*}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/User/asttt/code/actions/*}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{events/*/info.json\textasciigrave{} and \textasciigrave{}actions/*/info.json}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/asst\_cronXXXXX/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/asst\_cronXXXXX/email.txt}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/asst\_cronXXXXX/attachment.csv}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\$\{script\}\_log}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
There is no docstring.

\sphinxstepscope


\subsection{getAllPatients2.sh}
\label{\detokenize{Architecture/scripts/getAllPatients2:getallpatients2-sh}}\label{\detokenize{Architecture/scripts/getAllPatients2::doc}}
\sphinxAtStartPar
This script retrieves DICOM patient study information from a remote DICOM Query/Retrieve SCP server using the findscu command. It can pull data for a specified number of days (default 7000) and optionally filter by institution name. The script handles large datasets by implementing fallback strategies including date\sphinxhyphen{}based queries and patient ID pattern matching when bulk queries fail. Results are stored as individual DICOM files in organized directory structures under \sphinxcode{\sphinxupquote{/tmp/allPatients{[}InstitutionName{]}}}

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-e02d81cd95f1ec81fe87dc87063c918dd7baec04.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-82a41fd41e1df4290ce35672c731ed24becb5458.pdf}

\sphinxAtStartPar
Data pths
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/usr/share/dcmtk/dicom.dic}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output Paths:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/allPatients{[}InstitutionName{]}/}} \sphinxhyphen{} Main output directory

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/allPatients{[}InstitutionName{]}/all\_at\_once/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/allPatients{[}InstitutionName{]}/YYYYMMDD/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/allPatients{[}InstitutionName{]}/YYYYMMDD\_{[}pattern{]}/}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
We can provide an argument to this program, the maximum number of days we would like to pull. In general we might get away with a very short
period because new scans will come in as recent scans. But some test data migth be very old. So we should do one long run at night and short
runs during the day.

\sphinxAtStartPar
As a second argument allow a specific project name. The whole things takes too long right now. Treat some project as special here.

\sphinxstepscope


\subsection{heartbeat.sh}
\label{\detokenize{Architecture/scripts/heartbeat:heartbeat-sh}}\label{\detokenize{Architecture/scripts/heartbeat::doc}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{heartbeat.sh}} is a monitoring script that performs health checks on DICOM \sphinxcode{\sphinxupquote{storescp}} services by testing connectivity with \sphinxcode{\sphinxupquote{echoscu}}. If the connection test fails, it terminates unresponsive storescp processes and cleans up stuck \sphinxcode{\sphinxupquote{detectStudyArrival.sh}} jobs that have been running for over an hour. The script is designed to run via cron every minute and relies on external process managers like monit to restart killed services automatically.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-98bbdeb209feed75b18728cb2b743c447e954c92.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-4735fd479de217ff607586cfd7332bf61866954f.pdf}

\sphinxAtStartPar
Data paths:
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Input Folders/Files:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/config.json}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/logs/storescpd*.log}}

\end{itemize}

\end{description}

\item {} \begin{description}
\sphinxlineitem{Output Folders/Files:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/logs/heartbeat*.log}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/.pids/detectStudyArrival*.lock}}

\end{itemize}

\end{description}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
create a heart beat for the storescp

\sphinxAtStartPar
One way it can fail is if multiple associations are requested.

\sphinxAtStartPar
If the timeout happens the connection will be unusable afterwards.

\sphinxAtStartPar
Here we simply use echoscu to test the connection and if that fails we will kill a running storescp (hoping that monit will start it again).

\sphinxAtStartPar
In order to activate put this into the crontab of processing (every minute):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
*/1 * * * * /usr/bin/nice \PYGZhy{}n 3 /var/www/html/server/bin/heartbeat.sh
\end{sphinxVerbatim}

\sphinxstepscope


\subsection{parseAllPatients.sh}
\label{\detokenize{Architecture/scripts/parseAllPatients:parseallpatients-sh}}\label{\detokenize{Architecture/scripts/parseAllPatients::doc}}
\sphinxAtStartPar
This script processes DICOM files to extract unique patient IDs and retrieves study\sphinxhyphen{}level information for each patient from a DICOM Query/Retrieve service. It depends on output generated by \sphinxcode{\sphinxupquote{getAllPatients2.sh}} and uses DCMTK tools to parse DICOM files and query a remote PACS server. The script creates a directory structure organizing study information by patient ID and connects to a DICOM server to pull comprehensive study metadata including modalities, dates, and institutional information.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-fb0c7fe3e809188dafbcc72ff2d89b44fe6592f3.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-1c3893dec4badc488fddfa7dc5ca41f0b23d2a1e.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Directories:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/allPatients\{InstitutionName\}/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/usr/share/dcmtk/dicom.dic}}

\end{itemize}

\sphinxAtStartPar
Output Directories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/pullStudies\{InstitutionName\}/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/pullStudies\{InstitutionName\}/\{index\}/}}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
depends on output generated by getAllPatients2.sh

\sphinxAtStartPar
get list of optional findscu entries from \sphinxurl{http://dicom.nema.org/medical/dicom/current/output/html/part04.html\#sect\_C.6.1.1}

\sphinxstepscope


\subsection{populateAutoID.py}
\label{\detokenize{Architecture/scripts/populateAutoID:populateautoid-py}}\label{\detokenize{Architecture/scripts/populateAutoID::doc}}
\sphinxAtStartPar
This script automates the creation of transfer requests for medical imaging studies by checking auto\sphinxhyphen{}ID enabled projects in REDCap. It retrieves incoming DICOM data, generates or retrieves existing patient IDs using configurable naming patterns, and creates transfer requests for studies that don’t already have them. The script interfaces with multiple REDCap databases using different API tokens and calls an external \sphinxcode{\sphinxupquote{gen\sphinxhyphen{}id.py}} utility to generate new patient identifiers when needed.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-d380070ee69d8631f1ef801750cffee1e9f246d4.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-a715d091a7224b2de2146b45bb8c2ab8a43b04fc.pdf}

\sphinxAtStartPar
Data path
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Sources:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/bin/gen\sphinxhyphen{}id/gen\sphinxhyphen{}id.py}}

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Output Destinations:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Temporary files (created and cleaned up automatically)

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Check all auto\sphinxhyphen{}id projects and create new transfer requests for each

\sphinxstepscope


\subsection{populateIncoming.py}
\label{\detokenize{Architecture/scripts/populateIncoming:populateincoming-py}}\label{\detokenize{Architecture/scripts/populateIncoming::doc}}
\sphinxAtStartPar
This Python script populates the Study and Series information in the Incoming table in REDCap by processing DICOM metadata JSON files. It reads imaging series data from the filesystem, matches them against routing rules and coupling lists, and creates transfer requests for appropriate research projects. The script communicates with REDCap via API calls to store study/series metadata and generate transfer requests for data anonymization and distribution. It supports both incremental updates for new series and full project reimports via command\sphinxhyphen{}line arguments.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-b46491e72cf166b12da7152f672b53a22c8fed3b.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-4a069d75039dcffe9a31ed8544fb4d06d512ac11.pdf}

\sphinxAtStartPar
Data patsh
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/*/*.json}}

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Output Paths

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
RedCap endpoint

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
This program fills in the Study and Series information in the Incoming table in REDCap.

\sphinxAtStartPar
If a CouplingList entry exists its also adding a TransferRequest so that anonymizeAndSend can do its job.

\sphinxAtStartPar
TODO: support a new CouplingList entry even if there is already a TransferRequest done.

\sphinxstepscope


\subsection{populateProjects.py}
\label{\detokenize{Architecture/scripts/populateProjects:populateprojects-py}}\label{\detokenize{Architecture/scripts/populateProjects::doc}}
\sphinxAtStartPar
This Python script populates REDCap projects with imaging study information by retrieving transferred studies from incoming transfers and creating corresponding entries in target projects. The script fetches transfer data from a central REDCap database, matches it with project tokens, and creates participant records with associated imaging instrument data for each study that has been forwarded to PACS. It processes transfers either for all active projects or a specific project when the \textendash{}project parameter is provided, ensuring each study appears in its own REDCap project with proper repeat instance management.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-eb6156b441b58ea0248c415e9aa1c90d3a918ef3.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-60e48bc603089abdafcc83e8fe8235954d5c875f.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/bin/imagingProjects.json}}

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Output Destinations:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Multiple REDCap projects (determined dynamically from DataTransferProjects)

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Each study that has been forwarded to PACS should appear in its own REDCap project.

\sphinxAtStartPar
We can get a list of all transferred studies from incomings transfers. We get a token for the project and add the entry \sphinxhyphen{} if it does not exist yet.

\sphinxAtStartPar
For informatino to appear in the Imaging instrument you need to set it up as a repeating instrument for “Event 1” (not a repeating event!).

\sphinxAtStartPar
TODO: Without calling for specific projects does not work anymore. We need to get a list of all imaging projects and run them project by project.

\sphinxstepscope


\subsection{processSingleFile3.py}
\label{\detokenize{Architecture/scripts/processSingleFile3:processsinglefile3-py}}\label{\detokenize{Architecture/scripts/processSingleFile3::doc}}
\sphinxAtStartPar
ProcessSingleFile3.py is a daemon process that monitors a named pipe for DICOM file processing requests and creates organized directory structures with symbolic links. The daemon reads DICOM files, extracts header information including Siemens CSA headers, and organizes them into Study/Series hierarchies while generating JSON metadata files. It supports classification rules for automatic categorization of medical imaging data and handles structured reports separately from regular imaging data.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-84182a4eeccfe4d0988296dd4ec803ea0d62b3ff.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-e42a585c4c24787dc16b0878437cf50ab0483538.pdf}

\sphinxAtStartPar
Data pahts
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/config.json}} \sphinxhyphen{} Configuration file containing project settings classifyRules.json \sphinxhyphen{} Classification rules file (same directory as script)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/.processSingleFilePipe{[}projname{]}}} \sphinxhyphen{} Named pipe for receiving file processing requests

\item {} 
\sphinxAtStartPar
Source DICOM files (paths received via named pipe)

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/{[}StudyUID{]}/{[}SeriesUID{]}/}} \sphinxhyphen{} Organized DICOM structure with symbolic links

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/participants/{[}PatientID{]}/{[}StudyDate\_StudyTime{]}/}} \sphinxhyphen{} Patient\sphinxhyphen{}oriented  directory structure

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/srs/{[}Manufacturer{]}/{[}StudyUID{]}/}} \sphinxhyphen{} Structured reports directory

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/.arrived/}} \sphinxhyphen{} Touch files for series arrival detection

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/temp/}} \sphinxhyphen{} Temporary directory for atomic JSON file operations

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{../logs/processSingleFile{[}projname{]}.log}} \sphinxhyphen{} Log files

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{../.pids/processSingleFile{[}projname{]}.pid}} \sphinxhyphen{} Process ID files

\item {} 
\sphinxAtStartPar
Series JSON metadata files (.json files alongside DICOM directories)

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Create a daemon process that listens to send messages and reads a DICOM file,
extracts the header information and creates a Study/Series symbolic link structure.
\begin{description}
\sphinxlineitem{The parser for the Siemens CSA header have been adapted from}
\sphinxAtStartPar
\sphinxurl{https://scion.duhs.duke.edu/svn/vespa/tags/0\_1\_0/libduke\_mr/util\_dicom\_siemens.py}

\end{description}

\sphinxstepscope


\subsection{process\_tiff.sh}
\label{\detokenize{Architecture/scripts/process_tiff:process-tiff-sh}}\label{\detokenize{Architecture/scripts/process_tiff::doc}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{process\_tiff.sh}} script processes pathology image files (SVS and NDPI formats) by extracting metadata, anonymizing the images, and either importing them directly to a PACS system or storing them in a designated project folder. It handles two workflow paths: path\sphinxhyphen{}based imports where files are pseudonymized and stored locally, and DICOM conversion where files are converted to DICOM format, anonymized, and sent to a research PACS. The script processes JSON metadata files from REDCap uploads, extracts scanner information, and updates the database with processing results.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-a779c612ce1263c7538d1f964687a3cffaf859c6.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-298f29d5087410a7a7c53d5a8a9f6788bb030076.pdf}

\sphinxAtStartPar
Data pats
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Attach/uploads}} \sphinxhyphen{} Source JSON and image files

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Attach/storescu.cfg}} \sphinxhyphen{} DICOM configuration file

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Attach/uploads\_done}} \sphinxhyphen{} Processed JSON files Project\sphinxhyphen{}specific import folders (defined in JSON: project\_pat\_import\_folder)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export2/Attach/project\_cache/\{InstitutionName\}/}} \sphinxhyphen{} Optional project backup cache Research PACS server \sphinxhyphen{} DICOM storage destination

\end{itemize}

\item {} 
\sphinxAtStartPar
Temporary Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/}} \sphinxhyphen{} Temporary processing files Dynamically created temp directories for DICOM conversion and anonymization

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
To be updated.

\sphinxstepscope


\subsection{removeOldEntries.sh}
\label{\detokenize{Architecture/scripts/removeOldEntries:removeoldentries-sh}}\label{\detokenize{Architecture/scripts/removeOldEntries::doc}}
\sphinxAtStartPar
This bash script removes old entries from the \sphinxcode{\sphinxupquote{incoming.txt}} file based on timestamp comparison. It reads each line from the file, extracts the first field as a timestamp, and keeps only entries that are newer than 7 days (604800 seconds). The script uses a temporary file for atomic file operations and provides detailed logging of the cleanup process.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-343235089cb951e015fe67e0e564637100a34d23.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-995d74236ac9f4dce1d7ca5aaa0880b02f7cb30e.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input/Output Directories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Assign/incoming.txt}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/.tfile}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Rremove any old entries from incoming.txt

\sphinxstepscope


\subsection{resendProject.py}
\label{\detokenize{Architecture/scripts/resendProject:resendproject-py}}\label{\detokenize{Architecture/scripts/resendProject::doc}}
\sphinxAtStartPar
This Python script manages medical imaging transfer requests by checking REDCap database records for studies where the transfer date occurred before the request date. It retrieves transfer requests from a REDCap API, filters them based on date logic to identify studies that need to be resent, and generates JSON transfer request files for reprocessing. The script supports filtering by specific project names and creates uniquely named JSON files in a designated transfer requests directory.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-f2cf7eff2fc74c3581e531ac7f5eefa28547c295.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-1ec94094390ad9087525c1d77dae1f0360844117.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
REDCap database

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textasciigrave{}/home/processing/transfer\_requests/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\{study\_instance\_uid\}\_\{transfer\_project\_name\}\_\{transfer\_name\}\_\{redcap\_repeat\_instance\}.json}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
check all transfer requests that have already been done

\sphinxAtStartPar
if the send date is before the request date send again

\sphinxstepscope


\subsection{runOneJob.sh}
\label{\detokenize{Architecture/scripts/runOneJob:runonejob-sh}}\label{\detokenize{Architecture/scripts/runOneJob::doc}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{runOneJob.sh}} script processes workflow jobs by reading JSON job definitions from a queue file and executing containerized processing tasks using the ROR (Run on Request) system. It validates job parameters, creates working directories, executes Docker containers with specified images, and handles output validation by comparing DICOM StudyInstanceUIDs between input and output folders. Upon successful completion, it sends processed DICOM data to a FIONA storage system and optionally transfers results to REDCap, then removes completed jobs from the queue.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-cf47db8200324a581d7dcb5313d6b2aad8e0a560.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-cfedd27da447110d09a1491ce93af4c2bed391e9.pdf}

\sphinxAtStartPar
Data Paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Workflows/php/workflow\_joblist.jobs}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export/Workflows/\{PROJECT\}/ror/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/applications/Workflows/php/ror}}

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export/Workflows/\{PROJECT\}/ror/datajob\_\{JOB\_NUMBER\}\_\{IMAGE\_NAME\}\_*}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/export/Workflows/\{PROJECT\}/ror/datajob\_\{JOB\_NUMBER\}\_\{IMAGE\_NAME\}\_*\_output/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/\{DIRECTORY\_NAME\}run.log}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/home/processing/logs/Workflows\_RunOneJob.log}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Run a single job from the workflow\_joblist.jobs file. The file contains json code per line.

\sphinxAtStartPar
Need to run as user “processing” with flock.
\begin{description}
\sphinxlineitem{Example cron job:}\begin{description}
\sphinxlineitem{/usr/bin/flock \sphinxhyphen{}n /home/processing/.pids/Workflows\_RunOneJob.pid }
\sphinxAtStartPar
/var/www/html/applications/Workflows/php/runOneJob.sh \textgreater{}\textgreater{} /home/processing/logs/Workflows\_RunOneJob.log 2\textgreater{}\&1

\end{description}

\end{description}

\sphinxstepscope


\subsection{s2m.sh}
\label{\detokenize{Architecture/scripts/s2m:s2m-sh}}\label{\detokenize{Architecture/scripts/s2m::doc}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{s2m.sh}} script is a DICOM file transmission utility that sends DICOM directories to a local DICOM node using the dcmtk docker container. It supports sending individual directories, all studies for a specific \sphinxcode{\sphinxupquote{PatientID}}, or all studies from the last N days. The script reads DICOM network configuration from \sphinxcode{\sphinxupquote{/data/config/config.json}} and can handle project\sphinxhyphen{}specific routing. It includes fallback mechanisms using direct storescu commands if Docker\sphinxhyphen{}based transmission fails.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-1d99f6557763bf5964a0d20d763f5c9b751791d9.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-8ec457521a2d3c7899ad9cb53a9b77e104b6d5d7.pdf}

\sphinxAtStartPar
Data paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/config.json}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/archive/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/raw/*/*.json}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/usr/share/dcmtk/dicom.dic}}

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Remote destination for DICOM files, no local output files.

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip

\begin{itemize}
\item {} 
\sphinxAtStartPar
send to me (s2m)

\item {} 
\sphinxAtStartPar
Sends a DICOM directory using the dcmtk docker container from the local machine to the local DICOM node. This script can be used to re\sphinxhyphen{}classify DICOM files (creates /data/site/raw and /data/site/participant information).

\end{itemize}

\sphinxAtStartPar
Usage:
\begin{quote}

\sphinxAtStartPar
\# Send a single directory with DICOM files
s2m.sh \textless{}DICOM directory to send\textgreater{} {[}project{]}

\sphinxAtStartPar
\# Send all studies of a single PatientID
s2m.sh \textless{}PatientID\textgreater{} {[}project{]}

\sphinxAtStartPar
\# send all studies of the last 7 days
s2m.sh last 7 {[}project{]}
\end{quote}

\sphinxstepscope


\subsection{sendFiles.sh}
\label{\detokenize{Architecture/scripts/sendFiles:sendfiles-sh}}\label{\detokenize{Architecture/scripts/sendFiles::doc}}
\sphinxAtStartPar
This bash script automates the secure transfer of compressed DICOM and k\sphinxhyphen{}space data files from a local outbox directory to a remote DAIC (Data Analysis and Informatics Center) server using SFTP. The script compares local and remote MD5 checksums to avoid redundant transfers and implements file locking to prevent concurrent executions. It includes error handling for corrupt checksums, symbolic link conflicts, and disk space issues, with comprehensive logging of all operations.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-4aa99d633ed93d3ac00f07943f80d7ce723c016d.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-1765ba2a6f7e1693cc19bd530b6dbb75a1420d7e.pdf}

\sphinxAtStartPar
Data Paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Directories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/outbox/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/config.json}}

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Directories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/DAIC/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/logs/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/.pids/}}

\item {} 
\sphinxAtStartPar
Remote DAIC server endpoint (specified in config.json)

\end{itemize}

\item {} 
\sphinxAtStartPar
Temporary Directories:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/md5sums\_server\_XXXX}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
Example crontab entry that starts this script every 30 minutes

\begin{sphinxVerbatim}[commandchars=\\\{\}]
*/30 * * * * /usr/bin/nice \PYGZhy{}n 3 /var/www/html/server/bin/sendFiles.sh
\end{sphinxVerbatim}

\sphinxAtStartPar
Add the above line to your machine using:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYG{+w}{ }crontab\PYG{+w}{ }\PYGZhy{}e
\end{sphinxVerbatim}

\sphinxAtStartPar
This script is supposed to send compressed data files for DICOM and k\sphinxhyphen{}space to the DAIC endpoint using sftp. All data in the /data/outbox directory will be send using local and DAIC md5sum files.

\sphinxstepscope


\subsection{storectl.sh}
\label{\detokenize{Architecture/scripts/storectl:storectl-sh}}\label{\detokenize{Architecture/scripts/storectl::doc}}
\sphinxAtStartPar
This script manages a DICOM storage service (\sphinxcode{\sphinxupquote{storescp}} daemon) that receives medical imaging data on a specified port. It starts or stops the \sphinxcode{\sphinxupquote{storescpFIONA}} service for the  \sphinxcode{\sphinxupquote{processing}} user, which listens for incoming DICOM files and moves them to project\sphinxhyphen{}specific directories. The service can be controlled via an enabled/disabled flag file and supports multiple projects with different configurations (ABCD as default).

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-2db8802c18cde50a552b7f0d73de364716805f73.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-00e98f308b538fc5303c16d8c012c965211ee75a.pdf}

\sphinxAtStartPar
Data Paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/config/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/usr/share/dcmtk/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/}}

\end{itemize}

\item {} 
\sphinxAtStartPar
Output paths (data saved to):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/archive/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/data/site/.arrived/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/logs/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/var/www/html/server/.pids/}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
filename: storescpd

\sphinxAtStartPar
purpose: start storescp server for processing user at boot time to receive data
|         Move files to project specific file system

\sphinxAtStartPar
This system service will fail if a control file /data/enabled exists and  its first character is a “0”.

\sphinxAtStartPar
(Hauke Bartsch)

\sphinxstepscope


\subsection{whatIsInIDS7.py}
\label{\detokenize{Architecture/scripts/whatIsInIDS7:whatisinids7-py}}\label{\detokenize{Architecture/scripts/whatIsInIDS7::doc}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{whatIsInIDS7.py}} script processes DICOM medical imaging files from the IDS7 research PACS system and extracts metadata to populate a \sphinxcode{\sphinxupquote{REDCap}} database. It parses DICOM files using \sphinxcode{\sphinxupquote{dcm2json}}, extracts key study information (patient data, study details, series counts), handles duplicate studies by merging data, and uploads the processed information to REDCap via API calls. The script can process all studies or filter by institution name when provided as a command\sphinxhyphen{}line argument.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-85e87db37b7b120fbfb540f02cad88262dc89e0c.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-46fbbc979f0c7a2bdaef163e9c03aa3238759111.pdf}

\sphinxAtStartPar
Data Paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/tmp/pullStudies\{InstitutionName\}/*/*}} \sphinxhyphen{} DICOM files directory structure Command line arguments: sys.argv{[}1{]} (optional institution name filter)

\end{itemize}

\item {} 
\sphinxAtStartPar
Output paths:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{REDCap}} API endpoint

\end{itemize}

\item {} 
\sphinxAtStartPar
External dependencies:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dcm2json}} \sphinxhyphen{} DCMTK toolkit

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{/usr/share/dcmtk/dicom.dic}}

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
TODO: check if the number of study related series is correct (looks too large in Export app)

\sphinxstepscope


\subsection{whatIsNotInIDS7.py}
\label{\detokenize{Architecture/scripts/whatIsNotInIDS7:whatisnotinids7-py}}\label{\detokenize{Architecture/scripts/whatIsNotInIDS7::doc}}
\sphinxAtStartPar
This script cleans up a REDCap database by removing records that no longer exist in the research PACS system. It queries the REDCap project “whatIsInIDS7” to retrieve all stored records, then validates each record’s existence in the research PACS using DICOM findscu commands. Records that are not found in the PACS (indicated by zero SeriesInstanceUID occurrences) are marked for deletion and optionally removed from REDCap in batches of 200.

\sphinxAtStartPar
\sphinxstylestrong{Related Files}

\sphinxincludegraphics{mermaid-112387485dd38850e9a405edfd75818395f48330.pdf}

\sphinxAtStartPar
\sphinxstylestrong{Data Flow Diagram}

\sphinxincludegraphics{mermaid-3216f5903a07497760b6dfe4fe67be8301c7f226.pdf}

\sphinxAtStartPar
Data Paths
\begin{itemize}
\item {} 
\sphinxAtStartPar
Input Source:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{REDCap}} API

\end{itemize}

\item {} 
\sphinxAtStartPar
Output Destination:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{REDCap}} API

\end{itemize}

\end{itemize}


\bigskip\hrule\bigskip


\sphinxAtStartPar
Script docstring starts here —\textgreater{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}
\textendash{}\textgreater{}\textgreater{}

\sphinxAtStartPar
There is no docstring.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/anonymizeAndSend::doc}]{\sphinxcrossref{\DUrole{doc}{anonymizeAndSend.py}}}} \sphinxhyphen{} Processes imaging studies, performs anonymization, and sends them to research PACS

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/clearExports::doc}]{\sphinxcrossref{\DUrole{doc}{clearExports.sh}}}} \sphinxhyphen{} Removes old export files when storage reaches capacity thresholds

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/clearOldFiles::doc}]{\sphinxcrossref{\DUrole{doc}{clearOldFiles.sh}}}} \sphinxhyphen{} Removes old studies from \sphinxcode{\sphinxupquote{/data/site/archive}} when disk usage exceeds 80\%

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/clearStaleLinks::doc}]{\sphinxcrossref{\DUrole{doc}{clearStaleLinks.sh}}}} \sphinxhyphen{} Removes broken symbolic links and empty directories from data structures

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/createTransferRequests::doc}]{\sphinxcrossref{\DUrole{doc}{createTransferRequests.py}}}} \sphinxhyphen{} Generates transfer requests for studies that need anonymization and forwarding to research projects

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/createTransferRequestsForProcessed::doc}]{\sphinxcrossref{\DUrole{doc}{createTransferRequestsForProcessed.py}}}} \sphinxhyphen{} Handles transfer requests for processed/derived imaging data from workstations back to research PACS

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/createZipFileCmd::doc}]{\sphinxcrossref{\DUrole{doc}{createZipFileCmd.php}}}} \sphinxhyphen{}  Creates anonymized ZIP archives for research data distribution

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/cron::doc}]{\sphinxcrossref{\DUrole{doc}{cron.sh}}}} \sphinxhyphen{} Processes trigger\sphinxhyphen{}action pairs from JSON configuration files for event\sphinxhyphen{}driven automation

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/getAllPatients2::doc}]{\sphinxcrossref{\DUrole{doc}{getAllPatients2.sh}}}} \sphinxhyphen{} Retrieves patient and study information from research PACS using findscu

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/heartbeat::doc}]{\sphinxcrossref{\DUrole{doc}{heartbeat.sh}}}} \sphinxhyphen{} Checks DICOM service responsiveness and restarts failed components

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/parseAllPatients::doc}]{\sphinxcrossref{\DUrole{doc}{parseAllPatients.sh}}}} \sphinxhyphen{} Parses patient data retrieved by getAllPatients2.sh and extracts study\sphinxhyphen{}level metadata for REDCap import

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/populateAutoID::doc}]{\sphinxcrossref{\DUrole{doc}{populateAutoID.py}}}} \sphinxhyphen{}  Generates automatic participant IDs for projects using pseudonymized identifiers

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/populateIncoming::doc}]{\sphinxcrossref{\DUrole{doc}{populateIncoming.py}}}} \sphinxhyphen{} Processes incoming DICOM studies and creates metadata records in REDCap

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/populateProjects::doc}]{\sphinxcrossref{\DUrole{doc}{populateProjects.py}}}} \sphinxhyphen{} Populates individual research project databases with distributed data

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/processSingleFile3::doc}]{\sphinxcrossref{\DUrole{doc}{processSingleFile3.py}}}} \sphinxhyphen{} Extracts metadata from DICOM files and creates directory structures

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/process_tiff::doc}]{\sphinxcrossref{\DUrole{doc}{process\_tiff.sh}}}} \sphinxhyphen{} Converts whole slide imaging (WSI) files to DICOM format for pathology processing

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/removeOldEntries::doc}]{\sphinxcrossref{\DUrole{doc}{removeOldEntries.sh}}}} \sphinxhyphen{} Removes old entries from incoming data tracking files

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/resendProject::doc}]{\sphinxcrossref{\DUrole{doc}{resendProject.py}}}} \sphinxhyphen{} Handles re\sphinxhyphen{}transmission of studies when initial transfers fail or new data arrives

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/runOneJob::doc}]{\sphinxcrossref{\DUrole{doc}{runOneJob.sh}}}} \sphinxhyphen{} Processes containerized analysis jobs from job queue

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/s2m::doc}]{\sphinxcrossref{\DUrole{doc}{s2m.sh}}}} \sphinxhyphen{} Re\sphinxhyphen{}sends DICOM directories through the processing pipeline for re\sphinxhyphen{}classification

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/sendFiles::doc}]{\sphinxcrossref{\DUrole{doc}{sendFiles.sh}}}} \sphinxhyphen{} Uploads anonymized data to external research repositories via secure file transfer

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/storectl::doc}]{\sphinxcrossref{\DUrole{doc}{storectl.sh}}}} \sphinxhyphen{} Manages the main DICOM C\sphinxhyphen{}STORE receiver daemon

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/whatIsInIDS7::doc}]{\sphinxcrossref{\DUrole{doc}{whatIsInIDS7.py}}}}\sphinxhyphen{} Catalogs all studies present in the research imaging database

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Architecture/scripts/whatIsNotInIDS7::doc}]{\sphinxcrossref{\DUrole{doc}{whatIsNotInIDS7.py}}}}\sphinxhyphen{} Identifies and removes database entries for studies no longer in PACS

\end{enumerate}


\section{System setup pipeline}
\label{\detokenize{Architecture/index:system-setup-pipeline}}
\sphinxincludegraphics{mermaid-293d21e3f411ff6e1053094a0c6b79a3ac21b34c.pdf}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Dowlnoad image as PDF}}


\section{Data flow}
\label{\detokenize{Architecture/index:data-flow}}
\sphinxAtStartPar
Data flow scheme through the FIONA system, from initial DICOM reception to final transfer to research PACS, is presented below.

\sphinxincludegraphics{mermaid-33e3dcd610758409c1e1b74133e51b8214305c98.pdf}

\sphinxAtStartPar
Legend:




\section{Data Storage Structure}
\label{\detokenize{Architecture/index:data-storage-structure}}
\sphinxAtStartPar
The FIONA system uses a hierarchical storage structure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/data/
├── site/
│   ├── .arrived/         \PYGZsh{} Initial file reception
│   ├── archive/          \PYGZsh{} Raw DICOM storage
│   ├── raw/              \PYGZsh{} Processed DICOM files
│   └── output/           \PYGZsh{} Processing results
├── config/               \PYGZsh{} Configuration files
└── logs/                 \PYGZsh{} System logs
\end{sphinxVerbatim}

\sphinxAtStartPar
Project\sphinxhyphen{}specific directories follow the pattern:
/data\{PROJECT\}/site/…


\chapter{Contact Information}
\label{\detokenize{index:contact-information}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Website: \sphinxurl{https://fiona.ihelse.net}

\item {} 
\sphinxAtStartPar
Location: Haukeland University Hospital, Bergen, Norway

\end{itemize}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{genindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{modindex}}}

\item {} 
\sphinxAtStartPar
\DUrole{xref}{\DUrole{std}{\DUrole{std-ref}{search}}}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}